<!DOCTYPE html>
<html>
  <head>
    <title>Miles Liu - DECON Autonomous Disinfection Unit</title>
    <link
      rel="stylesheet"
      type="text/css"
      href="https://mliu59.github.io/main.css"
    />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="author" content="Miles Liu" />
  </head>

  <body>
    <h1>DECON Autonomous Disinfection Unit</h1>
    <section>
      <p>
        For the DECON project, our goal is to create an autonomous robotic
        platform that can safely disinfect high risk areas/items in public
        spaces of healthcare facilities , such as doorknobs, handlebars,
        light-switches, etc., to mitigate the risk of Healthcare Associated
        Infections (HAIs), all without disrupting regular healthcare workflow
        and traffic. To achieve this, our team designed a robotic platform
        proof-of-concept prototype that autonomously navigates itself between
        fixed ROIs, identifies target disinfection with computer vision , and
        executes a disinfectant spray and UVC radiation disinfection routine on
        fixed objects (doorknobs and light-switches), specifically targeting
        Methicilin-resistant Staphylococcus aureus (MRSA), which is one of the
        largest contributors to HAIs in the US. The project is a course project
        for Leading Innovation Design Team (taught by Dr. Charbel Rizk), and was
        sponsored by the
        <a href="https://www.jhuapl.edu/"
          >Johns Hopkins University Applied Physics Laboratory</a
        >.
      </p>
      <p>
        From left to right, our multidisciplinary team consists of Charbel Rizk
        (course instructor), Matt Chea, Neel Kovelamudi, me (Miles Liu), Thomas
        DiSorbo, Alia Nasrallah, Nafisa Ali Amir (not pictured), and Joshua Liu
        (not pictured).
      </p>
      <div>
        <a href="https://engineering.jhu.edu/lindt/">
          <img
            src="https://mliu59.github.io/decon/team.jpg"
            width="400"
            class="inlineRight"
          />
        </a>
      </div>
    </section>

    <section>
      <img
        src="https://mliu59.github.io/decon/robot.png"
        width="300"
        class="inlineRight"
      />
      <p>
        The robotic system is mainly divided into three main operational
        subsystems:
        <strong>Navigation, Detection, Disinfection. </strong>Robot operation is
        implemented through ROS.
      </p>
      <p>
        For <strong>Navigation</strong>, the robot relies on a LiDAR and SLAM to
        create a local map, and uses the Dynamic Window Approach to path find
        and navigate in a dynamic environment like that of a healthcare facility
        with average foot traffic and frequent obstacles. We used a
        <a href="https://www.robotis.us/turtlebot-3-waffle-pi/"
          >Robotis TurtleBot3 Waffle</a
        >
        as our base robot chassis.
      </p>
      <div>
        <img
          src="https://mliu59.github.io/decon/DWA.png"
          width="400"
          class="inlineRight"
        />
      </div>
      <p>
        For <strong>Detection</strong>, we want the robot to be able to
        identify, categorize, and localize disinfection targets (in our
        prototype setup: doorknobs and light-switches) within a specified ROI,
        after the robot has navigated to the general area. It will also need to
        recognize objects that are NOT to be subjected to disinfection for the
        robot to actively avoid, regardless of whether or not a target is
        nearby, such as human faces (would not want to flash high concentration
        UVC light at someone's face) and power outlets (due to the liquid
        disinfectant spray),
      </p>
      <p>
        To do this, we used a Zed stereo camera for object detection and
        localization. A Faster RCNN algorithm trained on the Open Images dataset
        from Google, augmented with our own target images, is used to recognize
        and categorize objects. The stereo camera can then triangulate the
        object's coordinates with respect to robot frame. That is then fed to
        the navigation algorithm or the robot arm to position the disinfection
        apparatus for a controlled disinfection on only the target object. We
        initially only used a Raspberry Pi for all computation, but eventually
        switched to an Nvidia Jetson TX2 to increase computational power for the
        robot.
      </p>
      <div>
        <img
          src="https://mliu59.github.io/decon/objectdetection.png"
          width="400"
          class="inlineRight"
        />
      </div>

      <p>
        For <strong>Disinfection</strong>, our goal is to deliver disinfection
        to the target object/area with high efficacy and low risk to nearby
        humans. For eliminating MRSA and other bacterium, we deployed a combined
        disinfection approach, using both commercial disinfectant spray as well
        as a UVC LED array emitting 250-260 nm wavelength UV light for
        disinfection. This specific UV light range can deactivate bacterial DNA
        to effectively stop bacterial growth. Our initial controlled laboratory
        tests found that UVC is effective in deactivating bacteria, but requires
        either prolonged exposure or high dosage beyond the power delivered by
        our LED array. The disinfectant spray is delivered through a peristaltic
        pump and an electronically controlled sprayer.
      </p>
      <p>
        To ensure safety, the entire disinfection system is contained within a
        disinfection cone, mounted on a 5-DoF robotic arm. When the disinfection
        mechanism is engaged (detection and navigation systems have reached a
        disinfection target within the arm's reach), the arm actuates to place
        the cone over the target object (doorknob and light-switches) against
        the wall, and prevents any UVC light from leaking out of the cone and
        potentially damaging the skin/eyes of anyone nearby. The disinfectant
        sprayer is placed at the base of the cone, and the UVC LEDs are radially
        positioned around the cone. The cone itself is reflective on the inside
        to maximize the amount of UVC delivered to the object surface.
      </p>
    </section>
    <section>
      <p>
        Nearing the end of the project academic year in March during the
        integration stages of the prototype, the COVID pandemic forced the team
        to work remotely on the project, which significantly hindered progress.
        Ultimately, we were able to develop a prototype that was able to
        navigate in a controlled environment, identify a small subset of target
        objects, and perform the disinfection routine. With more time, we would
        have wanted to further improve the robot's navigation efficiency,
        increase UVC disinfection efficacy, and make our solution more robust
        for dynamic environments. We were also hoping to conduct controlled
        testing for the system in actual healthcare facilities, but was not able
        to do so with the time and pandemic constraints.
      </p>
      <p>
        At the time of participation, I was a Mechanical Engineering
        undergraduate junior just starting a robotics curriculum, so I had
        limited opportunities to contribute efficiently to some of the robot
        subsystems, such as the computer vision and SLAM navigational
        components, as they were spearheaded by more advanced and specialized
        team members. However, looking back, the DECON project was an amazing
        learning experience for me. As my first year long robotics design
        project, being able to contribute to a sponsored real-world and highly
        relevant (almost ironically perfectly timed with COVID-19) project has
        taught me great deals in communication, project time & resource
        management, and working in highly multidisciplinary teams. On top of
        those, I was able to learn first hand the applications of various
        robotic sensors & actuators (LiDAR, stereo cameras, photometers, etc.),
        navigational and computational strategies (SLAM localization, DWA path
        planning, Computer vision and processing, etc.), and core robotic system
        components (ROS, power distribution, etc.).
      </p>
    </section>
  </body>
</html>
